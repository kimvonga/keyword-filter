{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# || Header ||\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "import linecache\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "keywords = ['Biosimilar', 'Otezla', 'Omecamtiv mecarbil', 'Aimovig', 'AMG 510', 'Neulasta', 'Onpro', 'M&A', 'Enbrel', 'Expense', 'Guidance',\n",
    "           'Payer mix', 'BiTE', 'Parsabiv', 'BD', 'margin', 'COVID Impact', 'Adaptive colab' ,'Inventory', 'Telemedicine', 'PD1', 'Aimovig',\n",
    "           'IL2', 'Drug pricing', 'Omecamtiv mecarbil', 'Tezepelumab', 'prolia', 'executive orders', 'neulasta onpro', 'Sotorasib', 'COVID',\n",
    "           'AMG510', 'Repatha', 'EVENITY']\n",
    "keywords = [i.lower() for i in keywords]    # make case insensitive\n",
    "keywords = list(set(keywords))              # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     1,
     21,
     39,
     54,
     81,
     120,
     140,
     168,
     196,
     234
    ]
   },
   "outputs": [],
   "source": [
    "# Notebook functions\n",
    "def pdfparser(data):\n",
    "    '''\n",
    "    Converts .pdf to one long string\n",
    "    '''\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'text'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "\n",
    "    return data\n",
    "\n",
    "def findCompany(my_list):\n",
    "    '''\n",
    "    Reads company name along with its abbreviation\n",
    "    Input should be first 100 words of file as a list of strings\n",
    "    Outputs as a string, e.g. 'Amgen, Inc, (AMGN)'\n",
    "    '''\n",
    "    for i in range(len(my_list)):\n",
    "        if len(my_list[i]) > 8:\n",
    "            if my_list[i][-4:].isnumeric() and my_list[i][-5] == '-' and my_list[i][-8:-5].isalpha():\n",
    "                date_ind = i\n",
    "                break\n",
    "    for i in range(date_ind, len(my_list)):\n",
    "        if my_list[i] in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "            quarter_ind = i\n",
    "            break\n",
    "\n",
    "    return ' '.join(my_list[date_ind+1: quarter_ind])\n",
    "\n",
    "def removePunctuation(my_list, ref=',.?!:;\\''):\n",
    "    '''\n",
    "    Given a list of strings, removes any punctuation found in the list regardless of its position\n",
    "    Returns as list of strings\n",
    "    '''\n",
    "    ref = list(ref)\n",
    "    \n",
    "    my_list = list(' '.join(my_list))\n",
    "    indices = [i for i in range(len(my_list)) if my_list[i] in ref]\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        del my_list[indices[-(i+1)]]\n",
    "        \n",
    "    return ''.join(my_list).split()\n",
    "\n",
    "def removeHeaderFooter(qa_paras, company):\n",
    "    '''\n",
    "    Removes header/footer found between pages.\n",
    "    Input should be list of paragraphs and company name, ref findCompany()\n",
    "    Outputs as list of paragraphs\n",
    "    '''\n",
    "    len_footer = len(company.split()) + 16\n",
    "\n",
    "    for i in range(len(qa_paras)):\n",
    "        my_para = qa_paras[i].split()\n",
    "\n",
    "        footer_end = 0\n",
    "        for j in range(len(my_para)):\n",
    "            if my_para[j] == 'FactSet':\n",
    "                if my_para[j+1] == 'CallStreet,' and my_para[j+2] == 'LLC':\n",
    "                    footer_end = j+2\n",
    "                    break\n",
    "\n",
    "        if footer_end > 0:\n",
    "            if len(my_para) == footer_end+1:\n",
    "                my_para = my_para[:footer_end-len_footer+1]\n",
    "            else:\n",
    "                my_para = my_para[:footer_end-len_footer+1] + my_para[footer_end+1:]\n",
    "            qa_paras[i] = ' '.join(my_para)\n",
    "\n",
    "    return qa_paras\n",
    "\n",
    "def checkKeyword(my_list, keyword):\n",
    "    '''\n",
    "    Checks if a given keyword or pair of keywords is found in the list of strings.\n",
    "    Returns either True or False\n",
    "    '''\n",
    "    def checkPlurals(my_str, keyword):\n",
    "        if keyword == my_str:\n",
    "            return True\n",
    "        elif keyword+'s' == my_str:\n",
    "            return True\n",
    "        elif keyword+'es' == my_str:\n",
    "            return True\n",
    "        elif keyword[:-1]+'ies' == my_str:\n",
    "            return True\n",
    "        elif keyword+'\\'s' == my_str:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    if len(keyword.split()) == 1:\n",
    "        for i in range(len(my_list)):\n",
    "            if checkPlurals(my_list[i], keyword):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    if len(keyword.split()) > 1:\n",
    "        len_pair = len(keyword.split())\n",
    "\n",
    "        my_dict = {}\n",
    "        for i in range(len_pair):\n",
    "            a = my_list[i:]+(len_pair-len(my_list[i:])%len_pair)*['padding']\n",
    "            my_dict['l'+str(i+1)] = np.reshape(a, (int(len(a)/len_pair), len_pair))\n",
    "\n",
    "        for i in range(len_pair):\n",
    "            for j in range(len(my_dict['l'+str(i+1)])):\n",
    "                if checkPlurals(' '.join(my_dict['l'+str(i+1)][j]), keyword):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "def searchPairs(my_list, search_pair):\n",
    "    '''\n",
    "    Searches list of words for pair of search words and returns the index\n",
    "        at which the pair begins\n",
    "    '''\n",
    "    len_pair = len(search_pair.split())\n",
    "    \n",
    "    my_dict = {}\n",
    "    for i in range(len_pair):\n",
    "        a = my_list[i:]+(len_pair-len(my_list[i:])%len_pair)*['padding']\n",
    "        my_dict['l'+str(i+1)] = np.reshape(a, (int(len(a)/len_pair), len_pair))\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(len_pair):\n",
    "        for j in range(len(my_dict['l'+str(i+1)])):\n",
    "            if ' '.join(my_dict['l'+str(i+1)][j]).lower() == search_pair.lower():\n",
    "                indices += [int(len_pair*j+i)]\n",
    "\n",
    "    return indices\n",
    "\n",
    "def pullCleanManagementDiscussion(my_dir, filename):\n",
    "    '''\n",
    "    Given directory of file and filename, reads .pdf and pulls management discussion section\n",
    "        while removing header/footer\n",
    "    Returns as list of paragraphs\n",
    "    '''\n",
    "    # pulls all text from pdf\n",
    "    words = pdfparser(my_dir+filename).split()\n",
    "    \n",
    "    # isolates Q&A section\n",
    "    management_section = words[searchPairs(words, 'management discussion section')[0]:searchPairs(words, 'question and answer section')[0]]\n",
    "\n",
    "    # separates Q&A section into paragraphs\n",
    "    management_paras = []\n",
    "    start, stop = 0, -1\n",
    "    for i in range(len(management_section)):\n",
    "        if management_section[i][:5] == '.....':\n",
    "            stop = i\n",
    "            management_paras += [' '.join(management_section[start:stop])]\n",
    "            start = stop + 1\n",
    "    del management_paras[0]\n",
    "\n",
    "    # Removes header/footer from Q&A paragraphs\n",
    "    company = findCompany(words[:50])\n",
    "    management_paras = removeHeaderFooter(management_paras, company)\n",
    "    \n",
    "    return management_paras\n",
    "\n",
    "def pullCleanQA(my_dir, filename):\n",
    "    '''\n",
    "    Given directory of file and filename, reads .pdf and pulls Q&A paragraphs\n",
    "        while removing header/footer\n",
    "    Returns as list of paragraphs\n",
    "    '''\n",
    "    # pulls all text from pdf\n",
    "    words = pdfparser(my_dir+filename).split()\n",
    "    \n",
    "    # isolates Q&A section\n",
    "    qa_section = words[searchPairs(words, 'question and answer section')[0]:]\n",
    "\n",
    "    # separates Q&A section into paragraphs\n",
    "    qa_paras = []\n",
    "    start, stop = 0, -1\n",
    "    for i in range(len(qa_section)):\n",
    "        if qa_section[i][:5] == '.....':\n",
    "            stop = i\n",
    "            qa_paras += [' '.join(qa_section[start:stop])]\n",
    "            start = stop + 1\n",
    "    del qa_paras[0]\n",
    "\n",
    "    # Removes header/footer from Q&A paragraphs\n",
    "    company = findCompany(words[:50])\n",
    "    qa_paras = removeHeaderFooter(qa_paras, company)\n",
    "    \n",
    "    return qa_paras\n",
    "\n",
    "def keywordsByQuestioner(qa_paras, keywords=keywords):\n",
    "    '''\n",
    "    Goes through Q&A and counts the number of questions that mention a given keyword\n",
    "    Returns two dictionaries where the value in key:value pairs are the names of the question asker\n",
    "    and in the second dictionary, the value is the number of people who have mentioned that keyword\n",
    "    '''\n",
    "    keyword_dict = {i:[] for i in keywords}\n",
    "\n",
    "    for i in range(len(qa_paras)):\n",
    "        my_para = removePunctuation([n.lower() for n in qa_paras[i].split()])\n",
    "\n",
    "        # determine if question paragraph and pull name\n",
    "        if 'q' not in my_para:\n",
    "            continue\n",
    "        else:\n",
    "            for j in range(len(my_para)):\n",
    "                if my_para[j] == 'q':\n",
    "                    name = 'q' if j==0 else ' '.join(qa_paras[i].split()[:j])\n",
    "                    break\n",
    "\n",
    "            # search through paragraph for keywords\n",
    "            for j in range(len(keywords)):\n",
    "                if checkKeyword(my_para, keywords[j]):\n",
    "                    keyword_dict[keywords[j]] += [name]\n",
    "\n",
    "    # remove duplicate names\n",
    "    keyword_count = {}\n",
    "    for i in range(len(keywords)):\n",
    "        keyword_dict[keywords[i]] = list(set(keyword_dict[keywords[i]]))\n",
    "        if len(keyword_dict[keywords[i]]) > 0:\n",
    "            keyword_count[keywords[i]] = len(keyword_dict[keywords[i]])\n",
    "        else:\n",
    "            del keyword_dict[keywords[i]]\n",
    "    keyword_dict = sorted(keyword_dict.items(), key=lambda x: -len(x[1]))\n",
    "    keyword_count = sorted(keyword_count.items(), key=lambda x: -x[1])\n",
    "    \n",
    "    return keyword_dict, keyword_count\n",
    "\n",
    "def keywordsByManagement(management_paras, keywords=keywords):\n",
    "    '''\n",
    "    Counts number of times a keyword is mentioned in the management discussion section\n",
    "    '''\n",
    "    keyword_count = {i:0 for i in keywords}\n",
    "    \n",
    "    my_list = removePunctuation(' '.join(management_paras).lower().split())\n",
    "    for i in range(len(keywords)):\n",
    "        if len(keywords[i].split()) == 1:\n",
    "            for j in range(len(my_list)):\n",
    "                if my_list[j].lower() == keywords[i]:\n",
    "                    keyword_count[keywords[i]] += 1\n",
    "        elif len(keywords[i].split()) > 1:\n",
    "            len_pair = len(keywords[i].split())\n",
    "            \n",
    "            for j in range(len_pair):\n",
    "                temp_list = my_list[j:]+(len_pair-len(my_list[j:])%len_pair)*['p@dding']\n",
    "                temp_list = np.reshape(temp_list, (int(len(temp_list)/len_pair), len_pair))\n",
    "                \n",
    "                for k in range(len(temp_list)):\n",
    "                    if ' '.join(temp_list[k]).lower() == keywords[i]:\n",
    "                        keyword_count[keywords[i]] += 1\n",
    "                        \n",
    "    for i in range(len(keywords)):\n",
    "        if keyword_count[keywords[i]] == 0:\n",
    "            del keyword_count[keywords[i]]\n",
    "    \n",
    "    return sorted(keyword_count.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = '/home/andy/OneDrive/Python/forMinh/pdfs/'   # directory containing .pdf file\n",
    "filename = 'AMGN 1Q 2020 EPS Call 4 30 2020.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_paras = pullCleanQA(my_dir, filename)\n",
    "qa_dict, qa_count = keywordsByQuestioner(qa_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', ['Matthew Harrison', 'q', 'Geoff Meacham', 'Carter Gould']),\n",
       " ('otezla', ['q', 'Geoff Meacham', 'Alethia Young']),\n",
       " ('biosimilar', ['q', 'Michael Schmidt', 'Ronny Gal']),\n",
       " ('amg510', ['Michael Yee', 'Terence Flynn']),\n",
       " ('margin', ['Jay Olson']),\n",
       " ('enbrel', ['q']),\n",
       " ('telemedicine', ['Robyn Karnauskas']),\n",
       " ('prolia', ['q']),\n",
       " ('evenity', ['Robyn Karnauskas']),\n",
       " ('bd', ['Geoff Meacham']),\n",
       " ('aimovig', ['Cory Kasimov']),\n",
       " ('drug pricing', ['q']),\n",
       " ('m&a', ['q'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', 4),\n",
       " ('otezla', 3),\n",
       " ('biosimilar', 3),\n",
       " ('amg510', 2),\n",
       " ('margin', 1),\n",
       " ('enbrel', 1),\n",
       " ('telemedicine', 1),\n",
       " ('prolia', 1),\n",
       " ('evenity', 1),\n",
       " ('bd', 1),\n",
       " ('aimovig', 1),\n",
       " ('drug pricing', 1),\n",
       " ('m&a', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "management_paras = pullCleanManagementDiscussion(my_dir, filename)\n",
    "management_count = keywordsByManagement(management_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('otezla', 12),\n",
       " ('guidance', 8),\n",
       " ('prolia', 6),\n",
       " ('evenity', 5),\n",
       " ('expense', 5),\n",
       " ('covid', 5),\n",
       " ('onpro', 3),\n",
       " ('bite', 3),\n",
       " ('amg 510', 3),\n",
       " ('aimovig', 3),\n",
       " ('enbrel', 2),\n",
       " ('telemedicine', 2),\n",
       " ('parsabiv', 2),\n",
       " ('omecamtiv mecarbil', 2),\n",
       " ('margin', 1),\n",
       " ('inventory', 1),\n",
       " ('biosimilar', 1),\n",
       " ('neulasta', 1),\n",
       " ('tezepelumab', 1),\n",
       " ('repatha', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "management_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qa_count.out', 'w') as file_out:\n",
    "    for pairs in qa_count:\n",
    "        file_out.write('%s,  %s\\n'%(pairs[0], pairs[1]))\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('management_count.out', 'w') as file_out:\n",
    "    for pairs in management_count:\n",
    "        file_out.write('%s,  %s\\n'%(pairs[0], pairs[1]))\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = '/home/andy/OneDrive/Python/forMinh/pdfs/'\n",
    "filename = 'CORRECTED TRANSCRIPT_ Amgen, Inc.(AMGN-US), Q2 2020 Earnings Call, 28-July-2020 5_00 PM ET.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "qa_paras = pullCleanQA(my_dir, filename)\n",
    "qa_dict, qa_count = keywordsByQuestioner(qa_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "management_paras = pullCleanManagementDiscussion(my_dir, filename)\n",
    "management_count = keywordsByManagement(management_paras)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
